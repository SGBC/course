{
    "docs": [
        {
            "location": "/",
            "text": "SGBC Bioinformatics Course\n\n\nThis website is a collection of lectures and tutorials given during the annual Bioinformatics course given at \nSLU\n by the \nSGBC\n.\n\n\nWelcome to the course!\n\n\nTable of Content\n\n\n\n\n\n\nWeek 1\n\n\n\n\nIntroduction to proteins analysis\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nIntroduction to the command-line\n\n\nCloud Computing\n\n\nInstalling Software\n\n\nVersion control with Git\n\n\nCommand-line Blast\n\n\nProject organisation\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nSequencing Technologies\n\n\nQuality Control\n\n\nDe-novo Genome Assembly\n\n\nAssembly Challenge\n\n\nGenome Annotation\n\n\nPan-genome Analysis\n\n\nNanopore Sequencing\n\n\n\n\n\n\n\n\nLicense\n\n\nUnless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit \nhttp://creativecommons.org/licenses/by/4.0/\n or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n\nContributors\n\n\nThe following people have contributed to these course material, in no particular order:\n\n\n\n\nHadrien Gourl\u00e9\n\n\nJuliette Hayer\n\n\nOskar Karlsson",
            "title": "Home"
        },
        {
            "location": "/#sgbc-bioinformatics-course",
            "text": "This website is a collection of lectures and tutorials given during the annual Bioinformatics course given at  SLU  by the  SGBC .  Welcome to the course!",
            "title": "SGBC Bioinformatics Course"
        },
        {
            "location": "/#table-of-content",
            "text": "Week 1   Introduction to proteins analysis     Week 2   Introduction to the command-line  Cloud Computing  Installing Software  Version control with Git  Command-line Blast  Project organisation     Week 3   Sequencing Technologies  Quality Control  De-novo Genome Assembly  Assembly Challenge  Genome Annotation  Pan-genome Analysis  Nanopore Sequencing",
            "title": "Table of Content"
        },
        {
            "location": "/#license",
            "text": "Unless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit  http://creativecommons.org/licenses/by/4.0/  or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "License"
        },
        {
            "location": "/#contributors",
            "text": "The following people have contributed to these course material, in no particular order:   Hadrien Gourl\u00e9  Juliette Hayer  Oskar Karlsson",
            "title": "Contributors"
        },
        {
            "location": "/proteins/",
            "text": "Introduction to protein sequences and structures analysis\n\n\n\n\n\nToolBox that could be useful for protein sequences analysis:\n\n\nhttp://expasy.org/\n\n\nhttp://www.uniprot.org/\n\n\nhttp://www.ebiokit.eu/\n\n\nhttp://npsa-pbil.ibcp.fr\n\n\nhttp://blast.ncbi.nlm.nih.gov/Blast.cgi\n\n\nhttps://www.ebi.ac.uk/interpro\n\n\nhttp://www.rcsb.org/pdb\n\n\n\n\n\nAfter cloning and sequencing of coding DNA, the sequence of the X\nprotein had been determined. The sequence of X is given here:\n\n\nLAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC\n\n\nIn normal conditions, this X protein is expressed but we have no idea\nabout it function. The goal of this practical work is to collect the\nmaximum of information about structure and function of the X protein.\n\n\nI - Search Patterns, Profiles\n\n\nA way to identify the function of X is to look if it contains signatures\n(pattern) of a function or a protein family.\n\n\n2 options:\n\n\nhttp://prosite.expasy.org/scanprosite/\n\n\nNPS@\n and follow the link \"ProScan: scan a\nsequence for sites/signatures against PROSITE database\" (activate:\nInclude documentation in result file).\n\n\n\n\nQuestion\n\n\n\n\n\n\nWhich signature(s) could you identify? Which specific features in\n    this protein?\n\n\n\n\n\n\nTry to change the parameters and comment the results.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nInterPro gives a summary of several methods. You can find it at the\n\nEBI\n.\n\n\n\n\nKeep the signatures that could attest the function in your notepad.\n\n\n\n\nWhat do you think about the function of X?\n\n\n\n\nII - Search homolog proteins with BLAST\n\n\n\n\n\n\nGo to the \nNCBI BLAST\n page\n\n\n\n\n\n\nChoose the Protein Blast (blastp)\n\n\n\n\n\n\nPaste your sequence\n\n\n\n\n\n\nSelect the Swissprot database\n\n\n\n\n\n\n\n\nQuestion\n\n\nDid you identify homologs? What are their function(s)?\n\n\n\n\nIII - Multiple sequences alignment\n\n\n\n\n\n\nSelect several homolog sequences from the Blast results.\n\n\n\n\n\n\nPerform a multiple sequence alignment (MSA) of these sequence using\n    Clustal Omega for example\n\n\n\n\n\n\nTry other MSA tools (for example Tcoffee and Muscle)\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you observe differences between the results obtained from\ndifferent algorithms?\n\n\nWhat can you observe in these MSAs?\n\n\n\n\nInfo\n: You could also retrieve the selected sequences in Fasta format\nand perform MSAs elsewhere\n\n\nClustal Omega and Muscle: available in Seaview alignment viewer\n\n\nTcoffee: \nhttp://tcoffee.vital-it.ch/apps/tcoffee/index.html\n\n\nOther tools: \nhttp://expasy.org/genomics/sequence_alignment\n\n\nIV - The Y protein\n\n\nAnother experiment had shown that the X protein was interacting\nspecifically with another protein: Y.\n\n\nAfter purification of the active Y protein, from the complex, a partial\nsequence of Y was obtained (by protein extremity sequencing).\n\n\nThe corresponding peptide could be:\n\n\nISGGD\n or \nISGGN\n\n\n1. Identification of the Y sequence using PROSITE patterns\n\n\n\n\n\n\nDesign the pattern (regular expression) corresponding to these\n    peptides.\n\n\n\n\n\n\nSearch the sequences containing this pattern in SwissProt using\n    \nPATTERN SEARCH\n at\n    SIB or\n    \nPATTINPROT\n\n    at NPS@.\n\n\n\n\n\n\nIf needed, use the help to design your pattern.\n\n\n\n\nQuestion\n\n\nHow many results do you get? How can you identify the right one?\n\n\n\n\nOnce the Y protein sequence identified, copy the FASTA sequence in your\nnotepad.\n\n\n2. Composition analysis\n\n\nAfter purification of the Y active protein, the amino-acid composition\nhas been determined (% of each aa in the protein) and is given in the\nfollowing table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n8.11\n\n\nF\n\n\n2.70\n\n\nL\n\n\n3.78\n\n\nR\n\n\n4.32\n\n\nX\n\n\n0\n\n\n\n\n\n\nB\n\n\n0\n\n\nG\n\n\n17.30\n\n\nM\n\n\n1.08\n\n\nS\n\n\n11.89\n\n\nY\n\n\n5.41\n\n\n\n\n\n\nC\n\n\n2.16\n\n\nH\n\n\n1.08\n\n\nN\n\n\n5.41\n\n\nT\n\n\n15.14\n\n\nZ\n\n\n0\n\n\n\n\n\n\nD\n\n\n3.78\n\n\nI\n\n\n3.78\n\n\nP\n\n\n2.70\n\n\nV\n\n\n7.57\n\n\n\n\n\n\n\n\n\n\nE\n\n\n1.08\n\n\nK\n\n\n0.54\n\n\nQ\n\n\n1.08\n\n\nW\n\n\n1.08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the composition of the sequence that you retrieve. Use\n    \nPROTPARAM\n or the tool\n    'Amino-acid composition' at \nNPS@\n\n\n\n\n\n\nCompare this computed composition with the composition of Y\n    experimentally determined.\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you observe differences? Explain.\n\n\n\n\n3. Search pattern in Y\n\n\nOnce the correct sequence of Y obtained, keep it in your notepad, you\nwill need it for the following analyses.\n\n\n\n\nQuestion\n\n\nIdentify the signatures (motifs, Pfam profiles) of Y using PROSCAN\nand/or Interpro.\n\n\n\n\n4. Identification of homologs to Y\n\n\n\n\n\n\nUse NCBI BLASTP or NPS@\n    \nBLASTP\n\n    against SwissProt database to search sequences similar to Y.\n\n\n\n\n\n\nUse PSI-BLAST (with SwissProt) to see if you can detect more distant\n    sequences.\n\n\n\n\n\n\nSelect sequences from BLAST and/or PSI-BLAST results to perform a\n    multiple sequence alignment.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nDid you observe difference in the results of BLAST and PSI-BLAST?\nComment.\n\n\nPropose a strategy to retrieve all the proteins having the same\ncatalytic activity as Y protein.\n\n\n\n\n\n\nV - Secondary structure prediction for X and Y\n\n\n\n\n\n\nGo to the \nconsensus secondary structure\n    prediction\n\n    page at NPS@.\n\n\n\n\n\n\nAnalyze the secondary structure of the protein Y. Include secondary\n    structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nConclude on the organization of secondary structures.\n\n\nPerform the same analysis for X protein.\n\n\n\n\n\n\nVI - Comparison with solved structures\n\n\n1. The Z protein\n\n\nThe structure of a protein Z has just been published. The sequence of\nprotein Z is shown below:\n\n\nIAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD\n\n\nITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT\n\n\nGGTTFYQPVTEALSAYGATVL\n\n\n\n\nQuestion\n\n\nCould you use this information for the study of protein Y? Make your\nown analysis.\n\n\n\n\n2. Find the correct structures\n\n\n\n\n\n\nDownload and install \nDeep-View -\n    SwissPDBViewer\n. You can\n    find the tutorial and user guide of DeepView\n    \nhere\n.\n\n\n\n\n\n\nDownload to the archive \nPDB_files_part6.zip\n and unzip it.\n\n\n\n\n\n\nYou might find 8 PDB files in the directory.\n\n\n\n\n\n\nOpen them with DeepView.\n\n\n\n\n\n\nDisplay the secondary structure representation mode (see part\n    VII-A-5 and/or the user guide).\n\n\n\n\n\n\n\n\nQuestion\n\n\nTry to identify the structures corresponding to X and Y proteins.\n\n\n\n\nVII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView)\n\n\n\n\n\n\nGo to the \nProtein Data Bank\n\n\n\n\n\n\nSearch and download the following PDB files: 1CRN, 1LDM.\n\n\n\n\n\n\nYou will visualize these protein structures using DeepView\n\n\nA - Analyze protein structures with DeepView\n\n\n1. Load a 3D structure\n\n\nFile => Open\n\n\nChoose the 1CRN.pdb file that you have downloaded from the PDB.\n\n\n2. Visualize the number of chains\n\n\nIs it only the protein or can we find ligands? Is it a monomer or a\npolymer?\n\n\n3. Visualize the general shape\n\n\nTry to get the actual space taken by the molecule. You need to use the\ncontrol panel and use the ':v' column to activate the space-filling\nspheres representation (+ menu Display > Render in solid 3D).\n\n\nTest also the Slab mode to visualize the space within the molecule:\nDisplay > Slab\n\n\n4. Display a distance between 2 atoms, angle between 3 atoms\n\n\nUse the graphical panel. You can now measure the real dimensions of your\nprotein\n\n\n5. Visualize secondary structure elements\n\n\nIn the control panel, activate \"ribbon\" (rbn). You can also color the\nmolecule by secondary structures.\n\n\n6. Visualize ligands (if there is any)\n\n\nSelect and color them. You could also remove the rest, or better, have a\nlook at the residues that are around those ligands (radius function in\nthe graphical panel).\n\n\n7. Analysis of other protein structures\n\n\nThe teacher will give PDB codes of other structures to analyze. Choose\nDeepView or Rasmol/Jmol to do so, that is up to you!\n\n\nB - Optional: if you want to use RasMol/Jmol\n\n\n1. Load a 3D structure\n\n\nFile => Open\n\n\nChoose the 1CRN.pdb file that you have downloaded from the PDB.\n\n\n\n\nHELP SECTION FOR RASMOL\n\n\nMolecule main moves with the mouse:\n\n\nLeft button: XY rotation\n\n\nLeft button + Shift: Zoom\n\n\nRight button: Translation\n\n\nRight button + Shift: Z rotation\n\n\nKeep the graphical window and the command (text) window on your screen\n(> \u200b\u200bis a command to type in the text window).\n\n\nFor each selection (SELECT command), the number of selected atoms\nappears in the text window. After you can apply an action to be able to\nvisualize the elements that you have selected (e.g. COLOR GREEN).\n\n\nCtrl+Z does not exist in Rasmol. You can type the command RESET.\n\n\nIf you want to come back in a standard representation of your molecule,\ntype:\n\n\nSELECT ALL\n\nCPK\n\n\n\n\n=> This will reset previous actions on representation modes (but keep\ncolors). CPK: space-filling spheres representation\n\n\nCOLOR CPK: colors \\'atom\\' objects by the atom (element) type\n\n\n\n\n\nHelp for Jmol:\n\n\nA lot of \"actions\" (color, selection...) are available by right clicking\non the main screen\n\n\nTo get the terminal window: menu File > Console\n\n\n\n\n\n2. Example: visualize the disulfide bonds\n\n\nType in the text window\n\n\nSELECT CYS\n\n\n\n\nThe text window \\\"answers\\\" 36 atoms selected (selected cysteine's\natoms)\n\n\nCOLOR GREEN\n\n\n\n\n\n\nObserve the graphics window.\n\n\n\n\nRESTRICT CYS\n\n\n\n\n\n\nCompare with the SELECT command\n\n\n\n\nHighlight the disulfide bonds:\n\n\nSSBONDS\n\nCOLOR YELLOW\n\nSSBONDS 75\n\nCOLOR CPK\n\n\n\n\n3. Visualize secondary structure elements\n\n\nSSBONDS OFF (remove SS bonds)\n\nSELECT ALL\n\nCARTOONS\n\nCOLOR STRUCTURE\n\n\n\n\n4. Display a distance between 2 atoms\n\n\nActivate the compute distance mode typing:\n\n\nSET PICKING DISTANCE\n\n\n\n\nThen, you can click the 2 atoms.\n\n\nYou can display angle values typing:\n\n\nSET PICKING ANGLE\n\n\n\n\nThen pick the 3 atoms\n\n\n5. Other useful commands\n\n\nSHOW SEQUENCE\n\nSHOW INFO\n\nSELECT ALL\n\nCPK ON\n\nRESTRICT NOT HOH (remove water molecules)\n\nCPK OFF\n\nHBONDS\n\nSELECT CYCLIC AND NOT PRO\n\nSTEREO ON\n\n\n\n\nTry them to better understand the Rasmol command language.\n\n\n6. Store a command script and reload it\n\n\nRepeat the actions described in paragraph 2\n\n\nWRITE SCRIPT MY_SCRIPT.SC\n\n\n\n\nExit the software (File => Quit)\n\n\nRestart the software\n\n\nSOURCE MY_SCRIPT.SC\n\n\n\n\n7. Select the atoms in a sphere\n\n\nFile => Close\n\n\nLoad the file 1LDM.pdb\n\n\nDiscover and analyze the molecule (number of channels, ligands, \netc\n.)\n\n\nTo select all the atoms in a 3\u00c5 radius sphere centered on a ligand\n(\ne.g.\n NAD)\n\n\nSELECT ALL\n\nCOLOR CHAIN\n\nSELECT WITHIN (3.0, NAD)\n\nCPK\n\n\n\n\nOption => Slab Mode (comment).",
            "title": "Proteins"
        },
        {
            "location": "/proteins/#introduction-to-protein-sequences-and-structures-analysis",
            "text": "ToolBox that could be useful for protein sequences analysis:  http://expasy.org/  http://www.uniprot.org/  http://www.ebiokit.eu/  http://npsa-pbil.ibcp.fr  http://blast.ncbi.nlm.nih.gov/Blast.cgi  https://www.ebi.ac.uk/interpro  http://www.rcsb.org/pdb   After cloning and sequencing of coding DNA, the sequence of the X\nprotein had been determined. The sequence of X is given here:  LAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC  In normal conditions, this X protein is expressed but we have no idea\nabout it function. The goal of this practical work is to collect the\nmaximum of information about structure and function of the X protein.",
            "title": "Introduction to protein sequences and structures analysis"
        },
        {
            "location": "/proteins/#i-search-patterns-profiles",
            "text": "A way to identify the function of X is to look if it contains signatures\n(pattern) of a function or a protein family.  2 options:  http://prosite.expasy.org/scanprosite/  NPS@  and follow the link \"ProScan: scan a\nsequence for sites/signatures against PROSITE database\" (activate:\nInclude documentation in result file).   Question    Which signature(s) could you identify? Which specific features in\n    this protein?    Try to change the parameters and comment the results.      Note  InterPro gives a summary of several methods. You can find it at the EBI .   Keep the signatures that could attest the function in your notepad.   What do you think about the function of X?",
            "title": "I - Search Patterns, Profiles"
        },
        {
            "location": "/proteins/#ii-search-homolog-proteins-with-blast",
            "text": "Go to the  NCBI BLAST  page    Choose the Protein Blast (blastp)    Paste your sequence    Select the Swissprot database     Question  Did you identify homologs? What are their function(s)?",
            "title": "II - Search homolog proteins with BLAST"
        },
        {
            "location": "/proteins/#iii-multiple-sequences-alignment",
            "text": "Select several homolog sequences from the Blast results.    Perform a multiple sequence alignment (MSA) of these sequence using\n    Clustal Omega for example    Try other MSA tools (for example Tcoffee and Muscle)     Question  Do you observe differences between the results obtained from\ndifferent algorithms?  What can you observe in these MSAs?   Info : You could also retrieve the selected sequences in Fasta format\nand perform MSAs elsewhere  Clustal Omega and Muscle: available in Seaview alignment viewer  Tcoffee:  http://tcoffee.vital-it.ch/apps/tcoffee/index.html  Other tools:  http://expasy.org/genomics/sequence_alignment",
            "title": "III - Multiple sequences alignment"
        },
        {
            "location": "/proteins/#iv-the-y-protein",
            "text": "Another experiment had shown that the X protein was interacting\nspecifically with another protein: Y.  After purification of the active Y protein, from the complex, a partial\nsequence of Y was obtained (by protein extremity sequencing).  The corresponding peptide could be:  ISGGD  or  ISGGN",
            "title": "IV - The Y protein"
        },
        {
            "location": "/proteins/#1-identification-of-the-y-sequence-using-prosite-patterns",
            "text": "Design the pattern (regular expression) corresponding to these\n    peptides.    Search the sequences containing this pattern in SwissProt using\n     PATTERN SEARCH  at\n    SIB or\n     PATTINPROT \n    at NPS@.    If needed, use the help to design your pattern.   Question  How many results do you get? How can you identify the right one?   Once the Y protein sequence identified, copy the FASTA sequence in your\nnotepad.",
            "title": "1. Identification of the Y sequence using PROSITE patterns"
        },
        {
            "location": "/proteins/#2-composition-analysis",
            "text": "After purification of the Y active protein, the amino-acid composition\nhas been determined (% of each aa in the protein) and is given in the\nfollowing table:                   A  8.11  F  2.70  L  3.78  R  4.32  X  0    B  0  G  17.30  M  1.08  S  11.89  Y  5.41    C  2.16  H  1.08  N  5.41  T  15.14  Z  0    D  3.78  I  3.78  P  2.70  V  7.57      E  1.08  K  0.54  Q  1.08  W  1.08         Compute the composition of the sequence that you retrieve. Use\n     PROTPARAM  or the tool\n    'Amino-acid composition' at  NPS@    Compare this computed composition with the composition of Y\n    experimentally determined.     Question  Do you observe differences? Explain.",
            "title": "2. Composition analysis"
        },
        {
            "location": "/proteins/#3-search-pattern-in-y",
            "text": "Once the correct sequence of Y obtained, keep it in your notepad, you\nwill need it for the following analyses.   Question  Identify the signatures (motifs, Pfam profiles) of Y using PROSCAN\nand/or Interpro.",
            "title": "3. Search pattern in Y"
        },
        {
            "location": "/proteins/#4-identification-of-homologs-to-y",
            "text": "Use NCBI BLASTP or NPS@\n     BLASTP \n    against SwissProt database to search sequences similar to Y.    Use PSI-BLAST (with SwissProt) to see if you can detect more distant\n    sequences.    Select sequences from BLAST and/or PSI-BLAST results to perform a\n    multiple sequence alignment.     Question   Did you observe difference in the results of BLAST and PSI-BLAST?\nComment.  Propose a strategy to retrieve all the proteins having the same\ncatalytic activity as Y protein.",
            "title": "4. Identification of homologs to Y"
        },
        {
            "location": "/proteins/#v-secondary-structure-prediction-for-x-and-y",
            "text": "Go to the  consensus secondary structure\n    prediction \n    page at NPS@.    Analyze the secondary structure of the protein Y. Include secondary\n    structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96).     Question   Conclude on the organization of secondary structures.  Perform the same analysis for X protein.",
            "title": "V - Secondary structure prediction for X and Y"
        },
        {
            "location": "/proteins/#vi-comparison-with-solved-structures",
            "text": "",
            "title": "VI - Comparison with solved structures"
        },
        {
            "location": "/proteins/#1-the-z-protein",
            "text": "The structure of a protein Z has just been published. The sequence of\nprotein Z is shown below:  IAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD  ITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT  GGTTFYQPVTEALSAYGATVL   Question  Could you use this information for the study of protein Y? Make your\nown analysis.",
            "title": "1. The Z protein"
        },
        {
            "location": "/proteins/#2-find-the-correct-structures",
            "text": "Download and install  Deep-View -\n    SwissPDBViewer . You can\n    find the tutorial and user guide of DeepView\n     here .    Download to the archive  PDB_files_part6.zip  and unzip it.    You might find 8 PDB files in the directory.    Open them with DeepView.    Display the secondary structure representation mode (see part\n    VII-A-5 and/or the user guide).     Question  Try to identify the structures corresponding to X and Y proteins.",
            "title": "2. Find the correct structures"
        },
        {
            "location": "/proteins/#vii-tridimensional-protein-structure-play-with-3d-structures-using-swisspdbviewer-deepview",
            "text": "Go to the  Protein Data Bank    Search and download the following PDB files: 1CRN, 1LDM.    You will visualize these protein structures using DeepView",
            "title": "VII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView)"
        },
        {
            "location": "/proteins/#a-analyze-protein-structures-with-deepview",
            "text": "",
            "title": "A - Analyze protein structures with DeepView"
        },
        {
            "location": "/proteins/#1-load-a-3d-structure",
            "text": "File => Open  Choose the 1CRN.pdb file that you have downloaded from the PDB.",
            "title": "1. Load a 3D structure"
        },
        {
            "location": "/proteins/#2-visualize-the-number-of-chains",
            "text": "Is it only the protein or can we find ligands? Is it a monomer or a\npolymer?",
            "title": "2. Visualize the number of chains"
        },
        {
            "location": "/proteins/#3-visualize-the-general-shape",
            "text": "Try to get the actual space taken by the molecule. You need to use the\ncontrol panel and use the ':v' column to activate the space-filling\nspheres representation (+ menu Display > Render in solid 3D).  Test also the Slab mode to visualize the space within the molecule:\nDisplay > Slab",
            "title": "3. Visualize the general shape"
        },
        {
            "location": "/proteins/#4-display-a-distance-between-2-atoms-angle-between-3-atoms",
            "text": "Use the graphical panel. You can now measure the real dimensions of your\nprotein",
            "title": "4. Display a distance between 2 atoms, angle between 3 atoms"
        },
        {
            "location": "/proteins/#5-visualize-secondary-structure-elements",
            "text": "In the control panel, activate \"ribbon\" (rbn). You can also color the\nmolecule by secondary structures.",
            "title": "5. Visualize secondary structure elements"
        },
        {
            "location": "/proteins/#6-visualize-ligands-if-there-is-any",
            "text": "Select and color them. You could also remove the rest, or better, have a\nlook at the residues that are around those ligands (radius function in\nthe graphical panel).",
            "title": "6. Visualize ligands (if there is any)"
        },
        {
            "location": "/proteins/#7-analysis-of-other-protein-structures",
            "text": "The teacher will give PDB codes of other structures to analyze. Choose\nDeepView or Rasmol/Jmol to do so, that is up to you!",
            "title": "7. Analysis of other protein structures"
        },
        {
            "location": "/proteins/#b-optional-if-you-want-to-use-rasmoljmol",
            "text": "",
            "title": "B - Optional: if you want to use RasMol/Jmol"
        },
        {
            "location": "/proteins/#1-load-a-3d-structure_1",
            "text": "File => Open  Choose the 1CRN.pdb file that you have downloaded from the PDB.   HELP SECTION FOR RASMOL  Molecule main moves with the mouse:  Left button: XY rotation  Left button + Shift: Zoom  Right button: Translation  Right button + Shift: Z rotation  Keep the graphical window and the command (text) window on your screen\n(> \u200b\u200bis a command to type in the text window).  For each selection (SELECT command), the number of selected atoms\nappears in the text window. After you can apply an action to be able to\nvisualize the elements that you have selected (e.g. COLOR GREEN).  Ctrl+Z does not exist in Rasmol. You can type the command RESET.  If you want to come back in a standard representation of your molecule,\ntype:  SELECT ALL\n\nCPK  => This will reset previous actions on representation modes (but keep\ncolors). CPK: space-filling spheres representation  COLOR CPK: colors \\'atom\\' objects by the atom (element) type   Help for Jmol:  A lot of \"actions\" (color, selection...) are available by right clicking\non the main screen  To get the terminal window: menu File > Console",
            "title": "1. Load a 3D structure"
        },
        {
            "location": "/proteins/#2-example-visualize-the-disulfide-bonds",
            "text": "Type in the text window  SELECT CYS  The text window \\\"answers\\\" 36 atoms selected (selected cysteine's\natoms)  COLOR GREEN   Observe the graphics window.   RESTRICT CYS   Compare with the SELECT command   Highlight the disulfide bonds:  SSBONDS\n\nCOLOR YELLOW\n\nSSBONDS 75\n\nCOLOR CPK",
            "title": "2. Example: visualize the disulfide bonds"
        },
        {
            "location": "/proteins/#3-visualize-secondary-structure-elements",
            "text": "SSBONDS OFF (remove SS bonds)\n\nSELECT ALL\n\nCARTOONS\n\nCOLOR STRUCTURE",
            "title": "3. Visualize secondary structure elements"
        },
        {
            "location": "/proteins/#4-display-a-distance-between-2-atoms",
            "text": "Activate the compute distance mode typing:  SET PICKING DISTANCE  Then, you can click the 2 atoms.  You can display angle values typing:  SET PICKING ANGLE  Then pick the 3 atoms",
            "title": "4. Display a distance between 2 atoms"
        },
        {
            "location": "/proteins/#5-other-useful-commands",
            "text": "SHOW SEQUENCE\n\nSHOW INFO\n\nSELECT ALL\n\nCPK ON\n\nRESTRICT NOT HOH (remove water molecules)\n\nCPK OFF\n\nHBONDS\n\nSELECT CYCLIC AND NOT PRO\n\nSTEREO ON  Try them to better understand the Rasmol command language.",
            "title": "5. Other useful commands"
        },
        {
            "location": "/proteins/#6-store-a-command-script-and-reload-it",
            "text": "Repeat the actions described in paragraph 2  WRITE SCRIPT MY_SCRIPT.SC  Exit the software (File => Quit)  Restart the software  SOURCE MY_SCRIPT.SC",
            "title": "6. Store a command script and reload it"
        },
        {
            "location": "/proteins/#7-select-the-atoms-in-a-sphere",
            "text": "File => Close  Load the file 1LDM.pdb  Discover and analyze the molecule (number of channels, ligands,  etc .)  To select all the atoms in a 3\u00c5 radius sphere centered on a ligand\n( e.g.  NAD)  SELECT ALL\n\nCOLOR CHAIN\n\nSELECT WITHIN (3.0, NAD)\n\nCPK  Option => Slab Mode (comment).",
            "title": "7. Select the atoms in a sphere"
        },
        {
            "location": "/unix/",
            "text": "Introduction to Unix\n\n\nMost of the introduction to Unix material can be found at \nhttps://software-carpentry.org\n\n\nMany thanks to them for existing!\n\n\nUseful resources\n\n\nBelow you will find links to various useful resources for learning or using the UNIX shell.\n\n\n\n\nLink to the course material from software carpentry\n\n\n\n\nreference of concepts and commands seen during the lesson\n\n\n\n\n\n\nshell commands explained\n - a website that shows the help text of any command\n\n\n\n\nawesome bash\n - an awesome list of resources about the bash shell\n\n\ntldp\n - the linux documentation project (the books can be hard to digest but are very thorough)",
            "title": "Unix"
        },
        {
            "location": "/unix/#introduction-to-unix",
            "text": "Most of the introduction to Unix material can be found at  https://software-carpentry.org  Many thanks to them for existing!",
            "title": "Introduction to Unix"
        },
        {
            "location": "/unix/#useful-resources",
            "text": "Below you will find links to various useful resources for learning or using the UNIX shell.   Link to the course material from software carpentry   reference of concepts and commands seen during the lesson    shell commands explained  - a website that shows the help text of any command   awesome bash  - an awesome list of resources about the bash shell  tldp  - the linux documentation project (the books can be hard to digest but are very thorough)",
            "title": "Useful resources"
        },
        {
            "location": "/cloud/",
            "text": "Introduction to Cloud Computing\n\n\nIn this lesson you'll learn how to connect and use a linux server.\nMost bioinformaticians worldwide connect daily to cloud computing services to perform their analyses.\n\n\nThere are several reasons for this.\nFirstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods.\nIt is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power.\n\n\nSecondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient.\n\n\nWhat is the cloud anyway?\n\n\nThe cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment.\n\n\n\n\nIn research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects.\nMost academic institutions or departments also have their own computing resources.\n\n\nPopular cloud/HPC services\n\n\n\n\n\n\nAcademic:\n\n\n\n\n\ud83c\uddf8\ud83c\uddea \nUPPMAX\n\n\n\ud83c\uddfa\ud83c\uddf8 \nJetstream\n\n\n\n\n\n\n\n\nCommercial:\n\n\n\n\namazon web services\n\n\ngoogle cloud\n\n\nmicrosoft azure\n\n\n\n\n\n\n\n\nFor this tutorial, we'll use Microsoft azure.\n\n\nConnecting to another computer\n\n\nConnecting to another computer is usually done using the \nSSH\n protocol, which is an encrypted way to connect over the network.\nBefore connecting to our cloud computers, we need to create them.\n\n\n\n\nNote\n\n\nWhile the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for.\n\n\n\n\n\n\nNote\n\n\nFor this course, your instructor will create a virtual instance on the Azure cloud for you\n\n\n\n\nAuthentication\n\n\nThere are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key.\nUsing a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys.\n\n\n\n\nNote\n\n\nHow do keys work?\nThe keys we will use to connect to our machine work by pairs: a \npublic\n key and a \nprivate\n key.\nAny machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer.\nWhen you try to connect to a machine and the two keys match, you successfully connect!\nSince your instructor will create a virtual machine for you, he will also provide you with a private key for this machine.\n\n\n\n\nPut the private key your instructor gave you in the \n~/.ssh\n folder:\n\n\nmkdir -p ~/.ssh\nchmod 700 ~/ssh\nmv ~/Downloads/azure_rsa ~/.ssh\nchmod 600 ~/.ssh/azure_rsa\n\n\n\n\nFirst connection\n\n\nNow you can connect to the virtual machine that was assigned to you\n\n\nssh -i .ssh/azure_rsa student@IP_ADDRESS\n\n\n\n\ndo not forget to replace \nIP_ADDRESS\n by your virtual machine ip in the above command!\n\n\nGetting around\n\n\nNow that you are connected to the cloud, there is a few things you should know.\n\n\n\n\nFor all intents and purposes it is \nalmost\n like being in the terminal of your own linux machine.\nAll commands we've seen during the \nunix shell\n lesson will work\n\n\nyou are administrator on your cloud machine. You have the power to break things...\n\n\n... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes\n\n\nto exit the virtual machine, press \n^D\n or type \nexit\n\n\n\n\nTransferring files\n\n\nOne thing that \nwill\n happen sooner while working in the cloud is that you will want to transfer files to or from your machine.\n\n\nThe command to transfer files over \nssh\n is very similar to \ncp\nand is called \nscp\n, for \nsecure\n copy.\n\n\nCopying a file from your computer to the server\n\n\nOn your computer, firstly create a file:\n\n\necho \"I will put that file on my cloud machine!\" > my_file.txt\n\n\n\n\nthen use \nscp\n to transfer the file\n\n\nscp my_file.txt student@IP_ADDRESS:/home/student/\n\n\n\n\nCopying a file from the server to your computer\n\n\nFirst, remove \nmy_file.txt\n from your local computer\n\n\nrm my_file.txt\n\n\n\n\nthen copy it back from the server\n\n\nscp student@IP_ADDRESS:/home/student/my_file.txt .",
            "title": "Cloud Computing"
        },
        {
            "location": "/cloud/#introduction-to-cloud-computing",
            "text": "In this lesson you'll learn how to connect and use a linux server.\nMost bioinformaticians worldwide connect daily to cloud computing services to perform their analyses.  There are several reasons for this.\nFirstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods.\nIt is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power.  Secondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient.",
            "title": "Introduction to Cloud Computing"
        },
        {
            "location": "/cloud/#what-is-the-cloud-anyway",
            "text": "The cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment.   In research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects.\nMost academic institutions or departments also have their own computing resources.",
            "title": "What is the cloud anyway?"
        },
        {
            "location": "/cloud/#popular-cloudhpc-services",
            "text": "Academic:   \ud83c\uddf8\ud83c\uddea  UPPMAX  \ud83c\uddfa\ud83c\uddf8  Jetstream     Commercial:   amazon web services  google cloud  microsoft azure     For this tutorial, we'll use Microsoft azure.",
            "title": "Popular cloud/HPC services"
        },
        {
            "location": "/cloud/#connecting-to-another-computer",
            "text": "Connecting to another computer is usually done using the  SSH  protocol, which is an encrypted way to connect over the network.\nBefore connecting to our cloud computers, we need to create them.   Note  While the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for.    Note  For this course, your instructor will create a virtual instance on the Azure cloud for you",
            "title": "Connecting to another computer"
        },
        {
            "location": "/cloud/#authentication",
            "text": "There are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key.\nUsing a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys.   Note  How do keys work?\nThe keys we will use to connect to our machine work by pairs: a  public  key and a  private  key.\nAny machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer.\nWhen you try to connect to a machine and the two keys match, you successfully connect!\nSince your instructor will create a virtual machine for you, he will also provide you with a private key for this machine.   Put the private key your instructor gave you in the  ~/.ssh  folder:  mkdir -p ~/.ssh\nchmod 700 ~/ssh\nmv ~/Downloads/azure_rsa ~/.ssh\nchmod 600 ~/.ssh/azure_rsa",
            "title": "Authentication"
        },
        {
            "location": "/cloud/#first-connection",
            "text": "Now you can connect to the virtual machine that was assigned to you  ssh -i .ssh/azure_rsa student@IP_ADDRESS  do not forget to replace  IP_ADDRESS  by your virtual machine ip in the above command!",
            "title": "First connection"
        },
        {
            "location": "/cloud/#getting-around",
            "text": "Now that you are connected to the cloud, there is a few things you should know.   For all intents and purposes it is  almost  like being in the terminal of your own linux machine.\nAll commands we've seen during the  unix shell  lesson will work  you are administrator on your cloud machine. You have the power to break things...  ... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes  to exit the virtual machine, press  ^D  or type  exit",
            "title": "Getting around"
        },
        {
            "location": "/cloud/#transferring-files",
            "text": "One thing that  will  happen sooner while working in the cloud is that you will want to transfer files to or from your machine.  The command to transfer files over  ssh  is very similar to  cp and is called  scp , for  secure  copy.",
            "title": "Transferring files"
        },
        {
            "location": "/cloud/#copying-a-file-from-your-computer-to-the-server",
            "text": "On your computer, firstly create a file:  echo \"I will put that file on my cloud machine!\" > my_file.txt  then use  scp  to transfer the file  scp my_file.txt student@IP_ADDRESS:/home/student/",
            "title": "Copying a file from your computer to the server"
        },
        {
            "location": "/cloud/#copying-a-file-from-the-server-to-your-computer",
            "text": "First, remove  my_file.txt  from your local computer  rm my_file.txt  then copy it back from the server  scp student@IP_ADDRESS:/home/student/my_file.txt .",
            "title": "Copying a file from the server to your computer"
        },
        {
            "location": "/software/",
            "text": "Installing software\n\n\nBioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field.\nTherefore new software as well as new versions of existing software are released on a regular basis.\n\n\nDuring this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment)\n\n\nCompiled and Interpreted languages\n\n\nProgramming languages in the bioinformatics world - and in general - can be separated in two categories: \nintepreted\n languages, and \ncompiled\n languages. While with \ninterpreted\n languages you write scripts, and execute them (as we saw with the \nbash\n scripts during the UNIX lesson) it is different for compiled languages: an extra step is required\n\n\nCompilation\n\n\nAs from \nWikipedia\n, compilation is the translation of source code into object code by a compiler.\n\n\nThat's right.\nThe extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s).\n\n\nThe big advantage of compiled languages is that they are much faster than interpreted languages.\nHowever, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language.\n\n\nThe most popular compiled language is the C programming language, which Linux is mainly written in.\n\n\nPackage Managers\n\n\nAll modern linux distributions come with a \npackage manager\n, i.e. a tool that automates installation of software.\nIn most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment\n\n\nLet us install our first package!\n\n\nThe package manager for \nUbuntu\n is called \nAPT\n. Like most package managers, the syntax will look like this:\n\n\n[package_manager] [action] [package_name]\n\n\n\n\nWe'll use apt to install a local version of \nncbi-blast\n that you've use previously.\n\n\nFirst we search if the package is available\n\n\napt search ncbi-blast\n\n\n\n\nThere seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one\n\n\napt show ncbi-blast+\n\n\n\n\nIt seems to be what we are looking for, we install it with:\n\n\napt install ncbi-blast+\n\n\n\n\n\n\nQuestion\n\n\nDid it work? What could have been wrong?\n\n\n\n\nYou should have gotten an error message asking if you are \nroot\n.\n\nThe user \nroot\n is the most powerful user in a linux system and usually has extra rights that a regular user does not have.\n\nTo install software in the default system location with apt, you have to have special permissions.\n\nWe can \"borrow\" those permissions from \nroot\n by prefixing our command with \nsudo\n.\n\n\nsudo apt install ncbi-blast+\n\n\n\n\nNow if you execute\n\n\nblastn -help\n\n\n\n\nit should print the (rather long) error message of the blastn command.\n\n\n\n\nQuestion\n\n\nWhy does blast has different executable?\n\nWhat is the difference between blastn and blastp?  \n\n\n\n\nDownloading and unpacking\n\n\nAlthough most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager.\n\n\nWe'll install \nspades\n, a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to:\n\n\n\n\ndownload the source code\n\n\ncompile the software\n\n\nmove it at the right place on our system\n\n\n\n\nWhich is quite cumbersome, especially the compilation.\nLuckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download.\n\n\nFirst let us create a directory for all our future installs:\n\n\nmkdir -p ~/install\ncd ~/install\n\n\n\n\nThe spades binaries are available on their website, \nhttp://cab.spbu.ru/software/spades/\n\n\nDownload them with\n\n\nwget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz\n\n\n\n\nand uncompress\n\n\ntar xvf SPAdes-3.11.1-Linux.tar.gz\n\n\n\n\ncd SPAdes-3.11.1-Linux/bin/\n\n\n\n\nand now if we execute \nspades.py\n\n\n./spades.py\n\n\n\n\nwe get the help of the spades assembler!\n\n\nA minor inconvenience is that right now\n\n\npwd\n# /home/hadrien/install/SPAdes-3.11.1-Linux/bin\n\n\n\n\nwe have to always go to this directory to run \nspades.py\n, or call the software with the full path.\nWe'd like to be able to execute \nspades\n from anywhere, like we do with \nls\n and \ncd\n.\n\n\nIn most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable: \n$PATH\n\n\nLet us take a look:\n\n\necho $PATH\n# /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\n\n\n\nTo make \nspades.py\n available from anywhere we have to put it in one of the above locations.\n\n\n\n\nNote\n\n\nWhen \napt\n installs software it usually places it in \n/usr/bin\n, which requires administration privileges.\nThis is why we needed \nsudo\n for installing packages earlier.\n\n\n\n\nmkdir -p ~/.local/bin\nmv * ~/.local/bin/\n\n\n\n\nEt voil\u00e0! Now you can execute \nspades.py\n from anywhere!\n\n\nInstalling from source\n\n\nFor some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system.\n\n\nThis is the case of \nsamtools\n per example. \nsamtools\n is one of the most popular bioinformatics software and allows you to deal with \nbam\n and \nsam\n files (more about that later)\n\n\nWe'll need a few things to be able to compile samtools, notably \nmake\n and a C compiler, \ngcc\n\n\nsudo apt install make gcc\n\n\n\n\nsamtools also need some libraries that are not installed by default on an ubuntu system.\n\n\nsudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev\n\n\n\n\nNow we can download and unpack the source code:\n\n\ncd ~/install\nwget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2\ntar xvf samtools-1.6.tar.bz2\ncd samtools-1.6\n\n\n\n\nCompiling software written in C usually follows the same 3 steps.\n\n\n\n\n./configure\n to configure the compilation options to our machine architecture\n\n\nwe run \nmake\n to compile the software\n\n\nwe run \nmake install\n to move the compiled binaries into a location in the \n$PATH\n\n\n\n\n./configure\nmake\nmake install\n\n\n\n\n\n\nWarning\n\n\nDid \nmake install\n succeed? Why not?\n\n\n\n\nAs we saw before, we need \nsudo\n to install packages to system locations with \napt\n.\n\nmake install\n follows the same principle and tries by default to install software in \n/usr/bin\n\n\nWe can change that default behavior by passing options to \nconfigure\n, but first we have to clean our installation:\n\n\nmake clean\n\n\n\n\nthan we can run configure, make and make install again\n\n\n./configure --prefix=/home/$(whoami)/.local/\nmake\nmake install\n\n\n\n\nsamtools\n\n\n\n\n\n\nQuestion\n\n\nThe bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!).\nNavigate to \nhttps://github.com/lh3/bwa\n then in release copy the link behind \nbwa-0.7.17.tar.bz2\n\n- Install bwa!\n\n\n\n\nInstalling python packages\n\n\nWhile compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug.\nFor theses reasons you'll often find many bioinformatics packages written in interpreted languages such as \npython\n or \nruby\n.\n\n\nWhile historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example:\n\n\n\n\nPython has \npip\n\n\nRuby has \ngem\n\n\nJavascript has \nnpm\n\n\n...\n\n\n\n\nMost of theses package managers have similar syntaxes.\nWe will focus on python here since it's one of the most popular languages in bioinformatics.\n\n\n\n\nNote\n\n\nYou will notice the absence of R here.\nR is mostly used interactively and installing packages in R will be part of the R part of the course.\n\n\n\n\nYour ubuntu comes with an old version of python. We start with installing a newer one\n\n\ncd ~/install\nwget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz\ntar xvf Python-3.6.4.tar.xz\ncd Python-3.6.4\n./configure --prefix=/home/$(whoami)/.local/\nmake -j2\nmake install\n\n\n\n\n\n\nQuestion\n\n\nWhat does the \nmake\n option \n-j2\n do?\n\n\n\n\nwhich python3\nwhich pip3\n\n\n\n\nWe now have the newest python installed.\n\n\nLet us install our first python package\n\n\npip3 install multiqc\n\n\n\n\nit should take a while and install \nmultiqc\n as well as all the necessary dependencies.\n\n\nto see if multiqc was properly installed:\n\n\nmultiqc -h\n\n\n\n\nExercises\n\n\nDuring the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks.\n\n\n\n\nTip\n\n\nMost software come with a file named \nINSTALL\n or \nREADME\n.\nSuch file usually contains instructions on how to install!\n\n\n\n\n\n\nNote\n\n\nunless indicated otherwise, try with \napt\n first\n\n\n\n\n\n\nNote\n\n\ndo not hesitate to ask your teacher for help!\n\n\n\n\nLet's install a few:\n\n\n\n\nfastqc\n\n\nscythe\n\n\nsickle\n\n\nbowtie2\n\n\nmegahit\n\n\nquast\n\n\nprokka",
            "title": "Installing Software"
        },
        {
            "location": "/software/#installing-software",
            "text": "Bioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field.\nTherefore new software as well as new versions of existing software are released on a regular basis.  During this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment)",
            "title": "Installing software"
        },
        {
            "location": "/software/#compiled-and-interpreted-languages",
            "text": "Programming languages in the bioinformatics world - and in general - can be separated in two categories:  intepreted  languages, and  compiled  languages. While with  interpreted  languages you write scripts, and execute them (as we saw with the  bash  scripts during the UNIX lesson) it is different for compiled languages: an extra step is required",
            "title": "Compiled and Interpreted languages"
        },
        {
            "location": "/software/#compilation",
            "text": "As from  Wikipedia , compilation is the translation of source code into object code by a compiler.  That's right.\nThe extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s).  The big advantage of compiled languages is that they are much faster than interpreted languages.\nHowever, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language.  The most popular compiled language is the C programming language, which Linux is mainly written in.",
            "title": "Compilation"
        },
        {
            "location": "/software/#package-managers",
            "text": "All modern linux distributions come with a  package manager , i.e. a tool that automates installation of software.\nIn most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment  Let us install our first package!  The package manager for  Ubuntu  is called  APT . Like most package managers, the syntax will look like this:  [package_manager] [action] [package_name]  We'll use apt to install a local version of  ncbi-blast  that you've use previously.  First we search if the package is available  apt search ncbi-blast  There seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one  apt show ncbi-blast+  It seems to be what we are looking for, we install it with:  apt install ncbi-blast+   Question  Did it work? What could have been wrong?   You should have gotten an error message asking if you are  root . \nThe user  root  is the most powerful user in a linux system and usually has extra rights that a regular user does not have. \nTo install software in the default system location with apt, you have to have special permissions. \nWe can \"borrow\" those permissions from  root  by prefixing our command with  sudo .  sudo apt install ncbi-blast+  Now if you execute  blastn -help  it should print the (rather long) error message of the blastn command.   Question  Why does blast has different executable? \nWhat is the difference between blastn and blastp?",
            "title": "Package Managers"
        },
        {
            "location": "/software/#downloading-and-unpacking",
            "text": "Although most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager.  We'll install  spades , a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to:   download the source code  compile the software  move it at the right place on our system   Which is quite cumbersome, especially the compilation.\nLuckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download.  First let us create a directory for all our future installs:  mkdir -p ~/install\ncd ~/install  The spades binaries are available on their website,  http://cab.spbu.ru/software/spades/  Download them with  wget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz  and uncompress  tar xvf SPAdes-3.11.1-Linux.tar.gz  cd SPAdes-3.11.1-Linux/bin/  and now if we execute  spades.py  ./spades.py  we get the help of the spades assembler!  A minor inconvenience is that right now  pwd\n# /home/hadrien/install/SPAdes-3.11.1-Linux/bin  we have to always go to this directory to run  spades.py , or call the software with the full path.\nWe'd like to be able to execute  spades  from anywhere, like we do with  ls  and  cd .  In most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable:  $PATH  Let us take a look:  echo $PATH\n# /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin  To make  spades.py  available from anywhere we have to put it in one of the above locations.   Note  When  apt  installs software it usually places it in  /usr/bin , which requires administration privileges.\nThis is why we needed  sudo  for installing packages earlier.   mkdir -p ~/.local/bin\nmv * ~/.local/bin/  Et voil\u00e0! Now you can execute  spades.py  from anywhere!",
            "title": "Downloading and unpacking"
        },
        {
            "location": "/software/#installing-from-source",
            "text": "For some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system.  This is the case of  samtools  per example.  samtools  is one of the most popular bioinformatics software and allows you to deal with  bam  and  sam  files (more about that later)  We'll need a few things to be able to compile samtools, notably  make  and a C compiler,  gcc  sudo apt install make gcc  samtools also need some libraries that are not installed by default on an ubuntu system.  sudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev  Now we can download and unpack the source code:  cd ~/install\nwget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2\ntar xvf samtools-1.6.tar.bz2\ncd samtools-1.6  Compiling software written in C usually follows the same 3 steps.   ./configure  to configure the compilation options to our machine architecture  we run  make  to compile the software  we run  make install  to move the compiled binaries into a location in the  $PATH   ./configure\nmake\nmake install   Warning  Did  make install  succeed? Why not?   As we saw before, we need  sudo  to install packages to system locations with  apt . make install  follows the same principle and tries by default to install software in  /usr/bin  We can change that default behavior by passing options to  configure , but first we have to clean our installation:  make clean  than we can run configure, make and make install again  ./configure --prefix=/home/$(whoami)/.local/\nmake\nmake install  samtools   Question  The bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!).\nNavigate to  https://github.com/lh3/bwa  then in release copy the link behind  bwa-0.7.17.tar.bz2 \n- Install bwa!",
            "title": "Installing from source"
        },
        {
            "location": "/software/#installing-python-packages",
            "text": "While compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug.\nFor theses reasons you'll often find many bioinformatics packages written in interpreted languages such as  python  or  ruby .  While historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example:   Python has  pip  Ruby has  gem  Javascript has  npm  ...   Most of theses package managers have similar syntaxes.\nWe will focus on python here since it's one of the most popular languages in bioinformatics.   Note  You will notice the absence of R here.\nR is mostly used interactively and installing packages in R will be part of the R part of the course.   Your ubuntu comes with an old version of python. We start with installing a newer one  cd ~/install\nwget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz\ntar xvf Python-3.6.4.tar.xz\ncd Python-3.6.4\n./configure --prefix=/home/$(whoami)/.local/\nmake -j2\nmake install   Question  What does the  make  option  -j2  do?   which python3\nwhich pip3  We now have the newest python installed.  Let us install our first python package  pip3 install multiqc  it should take a while and install  multiqc  as well as all the necessary dependencies.  to see if multiqc was properly installed:  multiqc -h",
            "title": "Installing python packages"
        },
        {
            "location": "/software/#exercises",
            "text": "During the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks.   Tip  Most software come with a file named  INSTALL  or  README .\nSuch file usually contains instructions on how to install!    Note  unless indicated otherwise, try with  apt  first    Note  do not hesitate to ask your teacher for help!   Let's install a few:   fastqc  scythe  sickle  bowtie2  megahit  quast  prokka",
            "title": "Exercises"
        },
        {
            "location": "/git/",
            "text": "Introduction to Git\n\n\nMost of the introduction to Git material can be found at \nhttps://software-carpentry.org\n\n\nMany thanks to them for existing!\n\n\nUseful resources\n\n\n\n\nLink to the course material from software carpentry\n\n\nreference of concepts and commands seen during the lesson\n\n\nThe official git website\n\n\nComparison of popular git hosting services - Medium article\n\n\nhttps://choosealicense.com",
            "title": "Git"
        },
        {
            "location": "/git/#introduction-to-git",
            "text": "Most of the introduction to Git material can be found at  https://software-carpentry.org  Many thanks to them for existing!",
            "title": "Introduction to Git"
        },
        {
            "location": "/git/#useful-resources",
            "text": "Link to the course material from software carpentry  reference of concepts and commands seen during the lesson  The official git website  Comparison of popular git hosting services - Medium article  https://choosealicense.com",
            "title": "Useful resources"
        },
        {
            "location": "/blast/",
            "text": "Command-line Blast\n\n\nInstalling blast\n\n\nWhile you should have installed blast during the \ninstalling software\n tutorial, you can copy/paste the code block below to reinstall it if needed\n\n\nsudo apt install ncbi-blast+\n\n\n\n\nGetting data\n\n\nWe will download some cows and human proteins from RefSeq\n\n\nwget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz\nwget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz\n\n\n\n\nBoth these files are compressed. They are not \ntar\n archives, like we encountered earlier, but \ngzip\n files.\nTo uncompress:\n\n\ngzip -d *.gz\n\n\n\n\nLet us take a look at the human file\n\n\nhead human.1.protein.faa\n\n\n\n\nBoth files contain protein sequences in the \nFASTA format\n\n\n\n\nQuestion\n\n\nHow many sequences do I have in each file?\n\n\n\n\nThe files are slightly too big for our first time blasting things at the command-line.\nLet's downsize the cow file\n\n\nhead -6 cow.1.protein.faa > cow.small.faa\n\n\n\n\nOur first blast\n\n\nNow we can blast these two cow sequences against the set of human sequences.\n\n\nFirst we need to build a blast database with our human sequences\n\n\nmakeblastdb -in human.1.protein.faa -dbtype prot\nls\n\n\n\n\nThe \nmakeblastdb\n produced a lot of extra files. Those files are indexes and necessary for blast to function.\n\n\nNow we can run blast\n\n\nblastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt\n\n\n\n\nWe can look at the results using \nless\n\n\nless cow_vs_human_blast_results.txt\n\n\n\n\nTo know about the various options that we can use with blastp:\n\n\nblastp -help\n\n\n\n\nand for easier reading\n\n\nblastp -help | less\n\n\n\n\n\n\nQuestion\n\n\nHow could I modify the previous blast command to filter the hits with an e-value of 1e-5\n\n\n\n\nBigger dataset\n\n\nNow that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one.\n\n\nhead -199 cow.1.protein.faa > cow.medium.faa\n\n\n\n\n\n\nQuestion\n\n\nHow many protein sequences does \ncow.medium.faa\n contain?\n\n\n\n\nWe run blast again\n\n\nblastp -query cow.medium.faa -db human.1.protein.faa \\\n    -out cow_vs_human_blast_results.tab -evalue 1e-5 \\\n    -outfmt 6 -max_target_seqs 1\n\n\n\n\n\n\nQuestion\n\n\nWhat do \n-outfmt\n and \n-max_target_seqs\n do?",
            "title": "Command-line Blast"
        },
        {
            "location": "/blast/#command-line-blast",
            "text": "",
            "title": "Command-line Blast"
        },
        {
            "location": "/blast/#installing-blast",
            "text": "While you should have installed blast during the  installing software  tutorial, you can copy/paste the code block below to reinstall it if needed  sudo apt install ncbi-blast+",
            "title": "Installing blast"
        },
        {
            "location": "/blast/#getting-data",
            "text": "We will download some cows and human proteins from RefSeq  wget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz\nwget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz  Both these files are compressed. They are not  tar  archives, like we encountered earlier, but  gzip  files.\nTo uncompress:  gzip -d *.gz  Let us take a look at the human file  head human.1.protein.faa  Both files contain protein sequences in the  FASTA format   Question  How many sequences do I have in each file?   The files are slightly too big for our first time blasting things at the command-line.\nLet's downsize the cow file  head -6 cow.1.protein.faa > cow.small.faa",
            "title": "Getting data"
        },
        {
            "location": "/blast/#our-first-blast",
            "text": "Now we can blast these two cow sequences against the set of human sequences.  First we need to build a blast database with our human sequences  makeblastdb -in human.1.protein.faa -dbtype prot\nls  The  makeblastdb  produced a lot of extra files. Those files are indexes and necessary for blast to function.  Now we can run blast  blastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt  We can look at the results using  less  less cow_vs_human_blast_results.txt  To know about the various options that we can use with blastp:  blastp -help  and for easier reading  blastp -help | less   Question  How could I modify the previous blast command to filter the hits with an e-value of 1e-5",
            "title": "Our first blast"
        },
        {
            "location": "/blast/#bigger-dataset",
            "text": "Now that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one.  head -199 cow.1.protein.faa > cow.medium.faa   Question  How many protein sequences does  cow.medium.faa  contain?   We run blast again  blastp -query cow.medium.faa -db human.1.protein.faa \\\n    -out cow_vs_human_blast_results.tab -evalue 1e-5 \\\n    -outfmt 6 -max_target_seqs 1   Question  What do  -outfmt  and  -max_target_seqs  do?",
            "title": "Bigger dataset"
        },
        {
            "location": "/project_organisation/",
            "text": "Project organization and management\n\n\nMost of the the project organization material can be found at \nhttps://software-carpentry.org\n and \nhttp://www.datacarpentry.org\n\n\nMany thanks to them for existing!\n\n\nStructure or architecture of a data science project\n\n\nSome good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute:\n\n\n\n\nCreate 3 or 4 different directories within you project directory (use \nmkdir\n):\n\n\ndata/\n for keeping the raw data\n\n\nresults/\n for all the outputs from the multiple analyses that you will perform\n\n\ndocs/\n for all the notes written about the analyses carried out (ex: \nhistory > 20180125.logs\n for the commands executed today)\n\n\nscripts/\n for all the scripts that you will use to produce the results\n\n\n\n\n\n\n\nNote\n\n\nYou should always have the raw data in (at least) one place and not modify them\n\n\n\n\nMore about data structure and metadata\n\n\n\n\n\n\ndirect link to the tutorial used fo the lesson: \nShell genomics: project organisation\n\n\n\n\n\n\ngood practice for the structure of data and metadata of a genomics project: \nOrganisation of genomics project\n\n\n\n\n\n\nSome extra material: \nSpreadsheet ecology lesson\n\n\n\n\n\n\nExercise\n\n\nThis exercise combines the knowledge you have acquired during the \nunix\n, \ngit\n and \nproject organisation\n lessons.\n\n\nYou have designed an experiment where you are studying the species and weight of animals caught in plots in a study area.\nData was collected by a third party a deposited in \nfigshare\n, a public database.\n\n\nOur goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!\n\n\nSet up\n\n\nFirst we go to our Desktop and create a project directory\n\n\ncd ~/Desktop\nmkdir 2018_animals\ncd 2018_animals\n\n\n\n\nand initialize 2018_animals as a git repository\n\n\ngit init\n\n\n\n\nAs we saw during the project organization tutorial, it is good practice to separate data, results and scripts.\nLet us create those three directories\n\n\nmkdir data results scripts\n\n\n\n\nDownloading the data\n\n\nFirst we go to our \ndata\n directory\n\n\ncd data\n\n\n\n\nthen we download our data file and give it a more appropriate name\n\n\nwget https://ndownloader.figshare.com/files/2292169\nmv 2292169 survey_data.csv\n\n\n\n\nSince we'll never modify our raw data file (or at least we \ndo not want to!\n) it is safer to remove the writing permissions\n\n\nchmod -w survey_data.csv\n\n\n\n\nAdditionally since we are now unable to modify it, we do not want to track it in our git repository.\nWe add a .gitignore and tell git to not track the \ndata/\n directory\n\n\nnano .gitignore\n\n\n\n\n\n\nNote\n\n\nwhat if my data is really big?\nUsually when you download data that is several gigabytes large, they will usually be compressed.\nYou learnt about compression during the \ninstalling software\n lesson.\n\n\n\n\nLet us look at the first few lines of our file:\n\n\nhead data/data_joined.csv\n\n\n\n\nOur data file is a \n.csv\n file, that is a file where fields are separated by commas \n,\n.\nEach row represent an animal that was caught in a plot, and each column contains information about that animal.\n\n\n\n\nQuestion\n\n\nHow many animals do we have?\n\n\n\n\nwc -l data/data_joined.csv\n# 34787 data/data_joined.csv\n\n\n\n\nIt seems that our dataset contains 34787 lines.\nSince each line is an animals, we caught a grand total of 34787 animals over the course of our study.\n\n\nOur first analysis script\n\n\nwe saw when we did the \nhead\n command that all 10 first plots captured rodents.\n\n\n\n\nQuestion\n\n\nIs rodent the only taxon that we have captured?\n\n\n\n\nIn our csv file, we can see that \"taxa\" is the 12th column.\nWe can print only that column using the \ncut\n command\n\n\ncut -d ',' -f 12 data/data_joined.csv | head\n\n\n\n\nWe still pipe in in head because we do not want to print 34787 line to our screen.\nAdditionally \nhead\n makes us notice that we still have the column header printed out\n\n\ncut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c\n\n\n\n\nBut while \nuniq\n is supposed to count all occurrence of a word, it only count similar \nadjacent\n occurrences.\nBefore counting, we need to sort our input:\n\n\ncut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c\n\n\n\n\nWe see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits!\n\n\nNow that we have a working one-liner, let us put it into a script\n\n\nnano scripts/taxa_count.sh\n\n\n\n\nand write\n\n\n# script that prints the count of species for csv files\ncut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c\n\n\n\n\nKeeping track of things\n\n\nNow keep track of your script in git\n\n\ngit add scripts/taxa_count.sh\ngit commit -m 'added taxa_count'\n\n\n\n\nas well as your gitignore\n\n\ngit add .gitignore\ngit commit -m 'added gitignore'\n\n\n\n\nSaving the result\n\n\nbash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt\n\n\n\n\ncat results/taxa_count.txt\n\n\n\n\ngit add results/taxa_count.txt\ngit commit -m 'added results of taxa_count.sh'\n\n\n\n\nImproving our script\n\n\nWe would also like to know the distribution of the numbers of animals caught in plots each year.\nThe year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file.\n\n\nWe can change our script to make it flexible so that the user can chose which columns they wishes to work on.\n\n\nnano scripts/taxa_count.sh\n\n\n\n\n# script that prints the count of occurrence in one column for csv files\ncut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c\n\n\n\n\nNow it doesn't make much sense to have it named \ntaxa_count.sh\n\n\nmv scripts/taxa_count.sh scripts/column_count.sh\n\n\n\n\nand let us not forget to keep track of our changes in git!\n\n\ngit add -A\ngit commit -m 'made script more flexible about which column to cut on'\n\n\n\n\n\n\nQuestion\n\n\nwhich year did we catch the most animals?\ntry to answer programmatically.\n\n\n\n\n\n\nQuestion\n\n\nsave the sorted output to a file in the \nresults\n directory and keep track of it in git.\n\n\n\n\nInvestigating further\n\n\nWe'd like to refine our animal count and knowing how many animals of each taxon were captured every year\n\n\nwe can use \ncut\n on several columns like this:\n\n\ncut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c\n\n\n\n\nNow that we are ahhpy with our one-liner, let us save it in a script:\n\n\nnano scripts/taxa_per_year.sh\n\n\n\n\nthen save the output to \nresults/taxa_per_year.txt\n\n\nbash scripts/taxa_per_year.sh > results/taxa_per_year.txt\n\n\n\n\n\n\nQuestion\n\n\nWhich year was the first reptile captured?\n\n\n\n\nThe next step would be to refine our analysis by year. We will save one individual output for each year count\n\n\nThe seq command\n\n\nTo perform what we want to do, we need to be able to loop over the years.\nThe \nseq\n command can help us with that.\n\n\nFirst we try\n\n\nseq 1 10\n\n\n\n\nthen\n\n\nseq 1997 2002\n\n\n\n\nand what about the span of years we are interested in?\n\n\nseq 1977 2002\n\n\n\n\nGreat! So now does it work with a for loop?\n\n\nfor year in $(seq 1977 2002)\n    do\n        echo $year\n    done\n\n\n\n\nIt does!\n\n\nBefore doing our analysis on each year, we still have to figure out how to do it on one year.\n\n\ngrep 1998 results/taxa_per_year.txt\n\n\n\n\n\"Grepping\" the year seems to work.\nNow we need to save it into a file containing the year\n\n\nFirst let's create a directory where to store our results\n\n\nmkdir results/years\n\n\n\n\nand we try to redirect our yearly count into a file\n\n\ngrep 1998 results/taxa_per_year.txt > results/years/1998-count.txt\n\n\n\n\nbash\ncat results/years/1998-count.txt\n\n\n\n\nIt seems to have worked.\nNow with the loop\n\n\nfor year in $(seq 1977 2002)\n    do\n        grep $year results/taxa_per_year.txt > results/years/$year-count.txt\n    done\n\n\n\n\nls results/years\n\n\n\n\n\n\nQuestion\n\n\nPut your loop in a script, and commit everything with \ngit",
            "title": "Project Organisation"
        },
        {
            "location": "/project_organisation/#project-organization-and-management",
            "text": "Most of the the project organization material can be found at  https://software-carpentry.org  and  http://www.datacarpentry.org  Many thanks to them for existing!",
            "title": "Project organization and management"
        },
        {
            "location": "/project_organisation/#structure-or-architecture-of-a-data-science-project",
            "text": "Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute:   Create 3 or 4 different directories within you project directory (use  mkdir ):  data/  for keeping the raw data  results/  for all the outputs from the multiple analyses that you will perform  docs/  for all the notes written about the analyses carried out (ex:  history > 20180125.logs  for the commands executed today)  scripts/  for all the scripts that you will use to produce the results    Note  You should always have the raw data in (at least) one place and not modify them",
            "title": "Structure or architecture of a data science project"
        },
        {
            "location": "/project_organisation/#more-about-data-structure-and-metadata",
            "text": "direct link to the tutorial used fo the lesson:  Shell genomics: project organisation    good practice for the structure of data and metadata of a genomics project:  Organisation of genomics project    Some extra material:  Spreadsheet ecology lesson",
            "title": "More about data structure and metadata"
        },
        {
            "location": "/project_organisation/#exercise",
            "text": "This exercise combines the knowledge you have acquired during the  unix ,  git  and  project organisation  lessons.  You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area.\nData was collected by a third party a deposited in  figshare , a public database.  Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!",
            "title": "Exercise"
        },
        {
            "location": "/project_organisation/#set-up",
            "text": "First we go to our Desktop and create a project directory  cd ~/Desktop\nmkdir 2018_animals\ncd 2018_animals  and initialize 2018_animals as a git repository  git init  As we saw during the project organization tutorial, it is good practice to separate data, results and scripts.\nLet us create those three directories  mkdir data results scripts",
            "title": "Set up"
        },
        {
            "location": "/project_organisation/#downloading-the-data",
            "text": "First we go to our  data  directory  cd data  then we download our data file and give it a more appropriate name  wget https://ndownloader.figshare.com/files/2292169\nmv 2292169 survey_data.csv  Since we'll never modify our raw data file (or at least we  do not want to! ) it is safer to remove the writing permissions  chmod -w survey_data.csv  Additionally since we are now unable to modify it, we do not want to track it in our git repository.\nWe add a .gitignore and tell git to not track the  data/  directory  nano .gitignore   Note  what if my data is really big?\nUsually when you download data that is several gigabytes large, they will usually be compressed.\nYou learnt about compression during the  installing software  lesson.   Let us look at the first few lines of our file:  head data/data_joined.csv  Our data file is a  .csv  file, that is a file where fields are separated by commas  , .\nEach row represent an animal that was caught in a plot, and each column contains information about that animal.   Question  How many animals do we have?   wc -l data/data_joined.csv\n# 34787 data/data_joined.csv  It seems that our dataset contains 34787 lines.\nSince each line is an animals, we caught a grand total of 34787 animals over the course of our study.",
            "title": "Downloading the data"
        },
        {
            "location": "/project_organisation/#our-first-analysis-script",
            "text": "we saw when we did the  head  command that all 10 first plots captured rodents.   Question  Is rodent the only taxon that we have captured?   In our csv file, we can see that \"taxa\" is the 12th column.\nWe can print only that column using the  cut  command  cut -d ',' -f 12 data/data_joined.csv | head  We still pipe in in head because we do not want to print 34787 line to our screen.\nAdditionally  head  makes us notice that we still have the column header printed out  cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c  But while  uniq  is supposed to count all occurrence of a word, it only count similar  adjacent  occurrences.\nBefore counting, we need to sort our input:  cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c  We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits!  Now that we have a working one-liner, let us put it into a script  nano scripts/taxa_count.sh  and write  # script that prints the count of species for csv files\ncut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c",
            "title": "Our first analysis script"
        },
        {
            "location": "/project_organisation/#keeping-track-of-things",
            "text": "Now keep track of your script in git  git add scripts/taxa_count.sh\ngit commit -m 'added taxa_count'  as well as your gitignore  git add .gitignore\ngit commit -m 'added gitignore'",
            "title": "Keeping track of things"
        },
        {
            "location": "/project_organisation/#saving-the-result",
            "text": "bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt  cat results/taxa_count.txt  git add results/taxa_count.txt\ngit commit -m 'added results of taxa_count.sh'",
            "title": "Saving the result"
        },
        {
            "location": "/project_organisation/#improving-our-script",
            "text": "We would also like to know the distribution of the numbers of animals caught in plots each year.\nThe year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file.  We can change our script to make it flexible so that the user can chose which columns they wishes to work on.  nano scripts/taxa_count.sh  # script that prints the count of occurrence in one column for csv files\ncut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c  Now it doesn't make much sense to have it named  taxa_count.sh  mv scripts/taxa_count.sh scripts/column_count.sh  and let us not forget to keep track of our changes in git!  git add -A\ngit commit -m 'made script more flexible about which column to cut on'   Question  which year did we catch the most animals?\ntry to answer programmatically.    Question  save the sorted output to a file in the  results  directory and keep track of it in git.",
            "title": "Improving our script"
        },
        {
            "location": "/project_organisation/#investigating-further",
            "text": "We'd like to refine our animal count and knowing how many animals of each taxon were captured every year  we can use  cut  on several columns like this:  cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c  Now that we are ahhpy with our one-liner, let us save it in a script:  nano scripts/taxa_per_year.sh  then save the output to  results/taxa_per_year.txt  bash scripts/taxa_per_year.sh > results/taxa_per_year.txt   Question  Which year was the first reptile captured?   The next step would be to refine our analysis by year. We will save one individual output for each year count",
            "title": "Investigating further"
        },
        {
            "location": "/project_organisation/#the-seq-command",
            "text": "To perform what we want to do, we need to be able to loop over the years.\nThe  seq  command can help us with that.  First we try  seq 1 10  then  seq 1997 2002  and what about the span of years we are interested in?  seq 1977 2002  Great! So now does it work with a for loop?  for year in $(seq 1977 2002)\n    do\n        echo $year\n    done  It does!  Before doing our analysis on each year, we still have to figure out how to do it on one year.  grep 1998 results/taxa_per_year.txt  \"Grepping\" the year seems to work.\nNow we need to save it into a file containing the year  First let's create a directory where to store our results  mkdir results/years  and we try to redirect our yearly count into a file  grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt  bash\ncat results/years/1998-count.txt  It seems to have worked.\nNow with the loop  for year in $(seq 1977 2002)\n    do\n        grep $year results/taxa_per_year.txt > results/years/$year-count.txt\n    done  ls results/years   Question  Put your loop in a script, and commit everything with  git",
            "title": "The seq command"
        },
        {
            "location": "/seq_tech/",
            "text": "Sequencing Technologies\n\n\n\n\nThis is an embedded \nMicrosoft Office\n presentation, powered by \nOffice Online\n.\n\n\n\n\n\nExercise\n\n\nBegin by reading \nGoodwin \net. al.\n 2016\n\n\nYou will be divided into four groups; each group will focus on one sequencing technology: Illumina, Ion Torrent, PacBio and Oxford Nanopore.\n\n\nEach group will present a short review of the technology, pros and cons as well as how they are used currently.\n\n\n\n\nWhat are the unique characteristics of the technology?\n\n\nWhat are the major use-cases for the technology?\n\n\nDescribe the significant landmarks of the technology.\n\n\n\n\nWrite a short presentation about the technology and present for the group. 10 min in total, 5-7 min presentation and 3 min questions.",
            "title": "Sequencing Technologies"
        },
        {
            "location": "/seq_tech/#sequencing-technologies",
            "text": "This is an embedded  Microsoft Office  presentation, powered by  Office Online .",
            "title": "Sequencing Technologies"
        },
        {
            "location": "/seq_tech/#exercise",
            "text": "Begin by reading  Goodwin  et. al.  2016  You will be divided into four groups; each group will focus on one sequencing technology: Illumina, Ion Torrent, PacBio and Oxford Nanopore.  Each group will present a short review of the technology, pros and cons as well as how they are used currently.   What are the unique characteristics of the technology?  What are the major use-cases for the technology?  Describe the significant landmarks of the technology.   Write a short presentation about the technology and present for the group. 10 min in total, 5-7 min presentation and 3 min questions.",
            "title": "Exercise"
        },
        {
            "location": "/tutorials/docs/qc/",
            "text": "Quality Control and Trimming\n\n\nLecture\n\n\n\n\n\nPractical\n\n\nIn this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.\n\n\nThe first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.\n\n\nDownloading the data\n\n\nThe raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA \nwebsite\n and search for the run with the accession SRR957824.\n\n\nHowever these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.\n\n\nFirst create a \ndata/\n directory in your home folder\n\n\nmkdir ~/data\n\n\n\n\nnow let's download the subset\n\n\ncd ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download\n\n\n\n\nLet\u2019s make sure we downloaded all of our data using md5sum.\n\n\nmd5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\nyou should see this\n\n\n1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz\n\n\n\n\nand now look at the file names and their size\n\n\nls -l\n\n\n\n\ntotal 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz\n\n\n\n\nThere are 500 000 paired-end reads taken randomly from the original data\n\n\nOne last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further\n\n\nchmod u-w *\n\n\n\n\nWorking Directory\n\n\nFirst we make a work directory: a directory where we can play around with a copy of the data without messing with the original\n\n\nmkdir ~/work\ncd ~/work\n\n\n\n\nNow we make a link of the data in our working directory\n\n\nln -s ~/data/* .\n\n\n\n\nThe files that we've downloaded are FASTQ files. Take a look at one of them with\n\n\nzless SRR957824_500K_R1.fastq.gz\n\n\n\n\n\n\nTip\n\n\nUse the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019\n\n\n\n\nYou can read more on the FASTQ format in the \nFile Formats\n lesson.\n\n\n\n\nQuestion\n\n\nWhere does the filename come from?\n\n\n\n\n\n\nQuestion\n\n\nWhy are there 1 and 2 in the file names?    \n\n\n\n\nFastQC\n\n\nTo check the quality of the sequence data we will use a tool called FastQC.\n\n\nFastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available \nhere\n.\n\n\nHowever, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files\n\n\nfastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\nand look what FastQC has produced\n\n\nls *fastqc*\n\n\n\n\nFor each file, FastQC has produced both a .zip archive containing all the plots, and a html report.\n\n\nDownload and open the html files with your favourite web browser.\n\n\nAlternatively you can look a these copies of them:\n\n\n\n\nSRR957824_500K_R1_fastqc.html\n\n\nSRR957824_500K_R2_fastqc.html\n\n\n\n\n\n\nQuestion\n\n\nWhat should you pay attention to in the FastQC report?\n\n\n\n\n\n\nQuestion\n\n\nWhich file is of better quality?\n\n\n\n\nPay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found \nhere\n.\nAlso, have a look at examples of a \ngood\n and a \nbad\n illumina read set for comparison.\n\n\nYou will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.\n\n\nScythe\n\n\nNow we'll do some trimming!\n\n\nScythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.\n\n\nThe first thing we need is the adapters to trim off\n\n\ncurl -O -J -L https://osf.io/v24pt/download\n\n\n\n\nNow we run scythe on both our read files\n\n\nscythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz\n\n\n\n\n\n\nQuestion\n\n\nWhat adapters do you use?\n\n\n\n\nSickle\n\n\nMost modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.\n\n\nWe will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.\n\n\nTo do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.\n\n\nTo run sickle\n\n\nsickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\\n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\\n    -s /dev/null -q 25\n\n\n\n\nwhich should output something like\n\n\nPE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)\n\n\n\n\nFastQC again\n\n\nRun fastqc again on the filtered reads\n\n\nfastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq\n\n\n\n\nand look at the reports\n\n\n\n\nSRR957824_trimmed_R1_fastqc.html\n\n\nSRR957824_trimmed_R2_fastqc.html\n\n\n\n\nMultiQC\n\n\nMultiQC\n is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.\n\n\nLet's run MultiQC in our current directory\n\n\nmultiqc .\n\n\n\n\nYou can download the report or view it by clickinh on the link below\n\n\n\n\nmultiqc_report.html\n\n\n\n\n\n\nQuestion\n\n\nWhat did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "Quality Control"
        },
        {
            "location": "/tutorials/docs/qc/#quality-control-and-trimming",
            "text": "",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/tutorials/docs/qc/#lecture",
            "text": "",
            "title": "Lecture"
        },
        {
            "location": "/tutorials/docs/qc/#practical",
            "text": "In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.  The first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.",
            "title": "Practical"
        },
        {
            "location": "/tutorials/docs/qc/#downloading-the-data",
            "text": "The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA  website  and search for the run with the accession SRR957824.  However these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.  First create a  data/  directory in your home folder  mkdir ~/data  now let's download the subset  cd ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download  Let\u2019s make sure we downloaded all of our data using md5sum.  md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  you should see this  1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz  and now look at the file names and their size  ls -l  total 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz  There are 500 000 paired-end reads taken randomly from the original data  One last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further  chmod u-w *",
            "title": "Downloading the data"
        },
        {
            "location": "/tutorials/docs/qc/#working-directory",
            "text": "First we make a work directory: a directory where we can play around with a copy of the data without messing with the original  mkdir ~/work\ncd ~/work  Now we make a link of the data in our working directory  ln -s ~/data/* .  The files that we've downloaded are FASTQ files. Take a look at one of them with  zless SRR957824_500K_R1.fastq.gz   Tip  Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019   You can read more on the FASTQ format in the  File Formats  lesson.   Question  Where does the filename come from?    Question  Why are there 1 and 2 in the file names?",
            "title": "Working Directory"
        },
        {
            "location": "/tutorials/docs/qc/#fastqc",
            "text": "To check the quality of the sequence data we will use a tool called FastQC.  FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available  here .  However, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files  fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  and look what FastQC has produced  ls *fastqc*  For each file, FastQC has produced both a .zip archive containing all the plots, and a html report.  Download and open the html files with your favourite web browser.  Alternatively you can look a these copies of them:   SRR957824_500K_R1_fastqc.html  SRR957824_500K_R2_fastqc.html    Question  What should you pay attention to in the FastQC report?    Question  Which file is of better quality?   Pay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found  here .\nAlso, have a look at examples of a  good  and a  bad  illumina read set for comparison.  You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.",
            "title": "FastQC"
        },
        {
            "location": "/tutorials/docs/qc/#scythe",
            "text": "Now we'll do some trimming!  Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.  The first thing we need is the adapters to trim off  curl -O -J -L https://osf.io/v24pt/download  Now we run scythe on both our read files  scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz   Question  What adapters do you use?",
            "title": "Scythe"
        },
        {
            "location": "/tutorials/docs/qc/#sickle",
            "text": "Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.  We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.  To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.  To run sickle  sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\\n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\\n    -s /dev/null -q 25  which should output something like  PE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)",
            "title": "Sickle"
        },
        {
            "location": "/tutorials/docs/qc/#fastqc-again",
            "text": "Run fastqc again on the filtered reads  fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq  and look at the reports   SRR957824_trimmed_R1_fastqc.html  SRR957824_trimmed_R2_fastqc.html",
            "title": "FastQC again"
        },
        {
            "location": "/tutorials/docs/qc/#multiqc",
            "text": "MultiQC  is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.  Let's run MultiQC in our current directory  multiqc .  You can download the report or view it by clickinh on the link below   multiqc_report.html    Question  What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "MultiQC"
        },
        {
            "location": "/assembly_challenge/",
            "text": "Assembly Challenge\n\n\nDuring this session, you will be asked to produce the best assembly possible of \nEscherichia coli\n str. K-12 MG1655.\n\n\nThere are various assemblers already installed on your virtual machines but feel free to try and install others.\nBelow you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools.\n\n\nGood luck!\n\n\nDownload the Data\n\n\ncurl -O -J -L https://osf.io/qszpw/download\ncurl -O -J -L https://osf.io/jaf6d/download\n\n\n\n\nAssemblers available\n\n\n\n\nmegahit\n\n\nSPAdes\n\n\nsga\n\n\nAbyss\n\n\nUnicycler\n\n\n\n\nQuality assessment\n\n\n\n\nquast\n\n\nbusco\n\n\nbowtie2\n\n\nmultiqc",
            "title": "Assembly Challenge"
        },
        {
            "location": "/assembly_challenge/#assembly-challenge",
            "text": "During this session, you will be asked to produce the best assembly possible of  Escherichia coli  str. K-12 MG1655.  There are various assemblers already installed on your virtual machines but feel free to try and install others.\nBelow you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools.  Good luck!",
            "title": "Assembly Challenge"
        },
        {
            "location": "/assembly_challenge/#download-the-data",
            "text": "curl -O -J -L https://osf.io/qszpw/download\ncurl -O -J -L https://osf.io/jaf6d/download",
            "title": "Download the Data"
        },
        {
            "location": "/assembly_challenge/#assemblers-available",
            "text": "megahit  SPAdes  sga  Abyss  Unicycler",
            "title": "Assemblers available"
        },
        {
            "location": "/assembly_challenge/#quality-assessment",
            "text": "quast  busco  bowtie2  multiqc",
            "title": "Quality assessment"
        },
        {
            "location": "/tutorials/docs/assembly/",
            "text": "De-novo Genome Assembly\n\n\nIn this practical we will perform the assembly of \nM. genitalium\n, a bacterium published in 1995 by Fraser et al in Science (\nabstract link\n).\n\n\nGetting the data\n\n\nM. genitalium\n was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession \nERR486840\n\n\nDownload the 2 fastq files associated with the run.\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz\n\n\n\n\nThe files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!\n\n\n\n\nQuestion\n\n\nHow many reads are in the files?\n\n\n\n\nDe-novo assembly\n\n\nWe will be using the MEGAHIT assembler to assemble our bacterium\n\n\nmegahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium\n\n\n\n\nThis will take a few minutes.\n\n\nThe result of the assembly is in the directory m_genitalium under the name \nfinal.contigs.fa\n\n\nLet's make a copy of it\n\n\ncp m_genitalium/final.contigs.fa m_genitalium.fasta\n\n\n\n\nand look at it\n\n\nhead m_genitalium.fasta\n\n\n\n\nQuality of the Assembly\n\n\nQUAST is a software evaluating the quality of genome assemblies by computing various metrics, including\n\n\nRun Quast on your assembly\n\n\nquast.py m_genitalium.fasta -o m_genitalium_report\n\n\n\n\nand take a look at the text report\n\n\ncat m_genitalium_report/report.txt\n\n\n\n\nYou should see something like\n\n\nAll statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00    \n\n\n\n\nwhich is a summary stats about our assembly.\nAdditionally, the file \nm_genitalium_report/report.html\n\n\nYou can either download it and open it in your own web browser, or we make it available for your convenience:\n\n\n\n\nm_genitalium_report/report.html\n\n\n\n\n\n\nNote\n\n\nN50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length\n\n\n\n\n\n\nQuestion\n\n\nHow well does the assembly total consensus size and coverage correspond to your earlier estimation?\n\n\n\n\n\n\nQuestion\n\n\nHow many contigs in total did the assembly produce?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the N50 of the assembly? What does this mean?\n\n\n\n\nFixing misassemblies\n\n\nPilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:\n\n\n\n\nSingle base differences\n\n\nSmall Indels\n\n\nLarger Indels or block substitution events\n\n\nGap filling\n\n\nIdentification of local misassemblies, including optional opening of new gaps\n\n\n\n\nPilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.\n\n\nBefore running Pilon itself, we have to align our reads against the assembly\n\n\nbowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam\n\n\n\n\nthen we run Pilon\n\n\npilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved\n\n\n\n\nwhich will correct eventual mismatches in our assembly and write the new improved assembly to \nm_genitalium_improved.fasta\n\n\nAssembly Completeness\n\n\nAlthough quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies!\n\n\nWe will run \nbusco\n to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality\n\n\nFirst we need to download and unpack the bacterial datasets used by \nbusco\n\n\nwget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz\ntar xzf bacteria_odb9.tar.gz\n\n\n\n\nthen we can run \nbusco\n with\n\n\nBUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome\n\n\n\n\n\n\nQuestion\n\n\nHow many marker genes has \nbusco\n found?",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#de-novo-genome-assembly",
            "text": "In this practical we will perform the assembly of  M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#getting-the-data",
            "text": "M. genitalium  was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession  ERR486840  Download the 2 fastq files associated with the run.  wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz  The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!   Question  How many reads are in the files?",
            "title": "Getting the data"
        },
        {
            "location": "/tutorials/docs/assembly/#de-novo-assembly",
            "text": "We will be using the MEGAHIT assembler to assemble our bacterium  megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium  This will take a few minutes.  The result of the assembly is in the directory m_genitalium under the name  final.contigs.fa  Let's make a copy of it  cp m_genitalium/final.contigs.fa m_genitalium.fasta  and look at it  head m_genitalium.fasta",
            "title": "De-novo assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#quality-of-the-assembly",
            "text": "QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including  Run Quast on your assembly  quast.py m_genitalium.fasta -o m_genitalium_report  and take a look at the text report  cat m_genitalium_report/report.txt  You should see something like  All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00      which is a summary stats about our assembly.\nAdditionally, the file  m_genitalium_report/report.html  You can either download it and open it in your own web browser, or we make it available for your convenience:   m_genitalium_report/report.html    Note  N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length    Question  How well does the assembly total consensus size and coverage correspond to your earlier estimation?    Question  How many contigs in total did the assembly produce?    Question  What is the N50 of the assembly? What does this mean?",
            "title": "Quality of the Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#fixing-misassemblies",
            "text": "Pilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:   Single base differences  Small Indels  Larger Indels or block substitution events  Gap filling  Identification of local misassemblies, including optional opening of new gaps   Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.  Before running Pilon itself, we have to align our reads against the assembly  bowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam  then we run Pilon  pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved  which will correct eventual mismatches in our assembly and write the new improved assembly to  m_genitalium_improved.fasta",
            "title": "Fixing misassemblies"
        },
        {
            "location": "/tutorials/docs/assembly/#assembly-completeness",
            "text": "Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies!  We will run  busco  to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality  First we need to download and unpack the bacterial datasets used by  busco  wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz\ntar xzf bacteria_odb9.tar.gz  then we can run  busco  with  BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome   Question  How many marker genes has  busco  found?",
            "title": "Assembly Completeness"
        },
        {
            "location": "/tutorials/docs/annotation/",
            "text": "Genome Annotation\n\n\nAfter you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.\n\n\nProkka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.\n\n\nProkka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using \nProdigal\n; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found \nhere\n.\n\n\nInput data\n\n\nProkka requires assembled contigs. You will need your best assembly from the assembly tutorial.\n\n\nAlternatively, you can download an assembly \nhere\n\n\nRunning prokka\n\n\nmodule load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta\n\n\n\n\nOnce Prokka has finished, examine each of its output files.\n\n\n\n\nThe GFF and GBK files contain all of the information about the features annotated (in different formats.)\n\n\nThe .txt file contains a summary of the number of features annotated.\n\n\nThe .faa file contains the protein sequences of the genes annotated.\n\n\nThe .ffn file contains the nucleotide sequences of the genes annotated.\n\n\n\n\nVisualising the annotation\n\n\nArtemis is a graphical Java program to browse annotated genomes. Download it \nhere\n and install it on your local computer.\n\n\nCopy the .gff file produced by prokka on your computer, and open it with artemis.\n\n\nYou will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:\n\n\n\n\nThere are 3 panels: feature map (top), sequence (middle), feature list (bottom)\n\n\nClick right-mouse-button on bottom panel and select Show products\n\n\nZooming is done via the verrtical scroll bars in the two top panels",
            "title": "Genome Annotation"
        },
        {
            "location": "/tutorials/docs/annotation/#genome-annotation",
            "text": "After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.  Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.  Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using  Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found  here .",
            "title": "Genome Annotation"
        },
        {
            "location": "/tutorials/docs/annotation/#input-data",
            "text": "Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial.  Alternatively, you can download an assembly  here",
            "title": "Input data"
        },
        {
            "location": "/tutorials/docs/annotation/#running-prokka",
            "text": "module load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta  Once Prokka has finished, examine each of its output files.   The GFF and GBK files contain all of the information about the features annotated (in different formats.)  The .txt file contains a summary of the number of features annotated.  The .faa file contains the protein sequences of the genes annotated.  The .ffn file contains the nucleotide sequences of the genes annotated.",
            "title": "Running prokka"
        },
        {
            "location": "/tutorials/docs/annotation/#visualising-the-annotation",
            "text": "Artemis is a graphical Java program to browse annotated genomes. Download it  here  and install it on your local computer.  Copy the .gff file produced by prokka on your computer, and open it with artemis.  You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:   There are 3 panels: feature map (top), sequence (middle), feature list (bottom)  Click right-mouse-button on bottom panel and select Show products  Zooming is done via the verrtical scroll bars in the two top panels",
            "title": "Visualising the annotation"
        },
        {
            "location": "/tutorials/docs/pan_genome/",
            "text": "Pan-Genome Analysis\n\n\nIn this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.\n\n\nThis tutorial is inspired from \nGenome annotation and Pangenome Analysis\n from the CBIB in Santiago, Chile\n\n\nGetting the data\n\n\nWe'll use six \nListeria monocytogenes\n genomes in this tutorial.\n\n\nwget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome\n\n\n\n\nThese genomes correspond to isolates of \nL. monocytogenes\n reported in\n\n\n\n\nXiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500\n\n\n\n\nThe six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):\n\n\n\n\n\n\n\n\nGenome Assembly\n\n\nGenome Accession\n\n\nGenotype\n\n\nSequenced by\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nGCA_000026705\n\n\nFM242711\n\n\ntype I\n\n\nInstitut_Pasteur\n\n\nFinished\n\n\n\n\n\n\nGCA_000008285\n\n\nAE017262\n\n\ntype I\n\n\nTIGR\n\n\nFinished\n\n\n\n\n\n\nGCA_000168815\n\n\nAATL00000000\n\n\ntype I\n\n\nBroad Institute\n\n\n79 contigs\n\n\n\n\n\n\nGCA_000196035\n\n\nAL591824\n\n\ntype II\n\n\nEuropean Consortium\n\n\nFinished\n\n\n\n\n\n\nGCA_000168635\n\n\nAARW00000000\n\n\ntype II\n\n\nBroad Institute\n\n\n25 contigs\n\n\n\n\n\n\nGCA_000021185\n\n\nCP001175\n\n\ntype III\n\n\nMSU\n\n\nFinished\n\n\n\n\n\n\n\n\nAnnotation of the genomes\n\n\nBy annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.\n\n\nprokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna\n\n\n\n\nAnnotate the 6 genomes, by replacing the \n-outdir\n and \n-locustag\n and \nfasta file\n accordingly.\n\n\nPan-genome analysis\n\n\nput all the .gff files in the same folder (e.g., ./gff) and run Roary\n\n\nroary -f results -e -n -v gff/*.gff\n\n\nRoary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the \nsummary_statistics.txt\n file.\n\n\nAdditionally, Roary produces a \ngene_presence_absence.csv\n file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\n\n\nPlotting the result\n\n\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.\n\n\nFirst, we need to generate a tree file from the alignment generated by Roary:\n\n\nFastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick\n\n\n\n\nThen we can plot the Roary results:\n\n\nroary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Pan-genome Analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#pan-genome-analysis",
            "text": "In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.  This tutorial is inspired from  Genome annotation and Pangenome Analysis  from the CBIB in Santiago, Chile",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#getting-the-data",
            "text": "We'll use six  Listeria monocytogenes  genomes in this tutorial.  wget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome  These genomes correspond to isolates of  L. monocytogenes  reported in   Xiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500   The six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):     Genome Assembly  Genome Accession  Genotype  Sequenced by  Status      GCA_000026705  FM242711  type I  Institut_Pasteur  Finished    GCA_000008285  AE017262  type I  TIGR  Finished    GCA_000168815  AATL00000000  type I  Broad Institute  79 contigs    GCA_000196035  AL591824  type II  European Consortium  Finished    GCA_000168635  AARW00000000  type II  Broad Institute  25 contigs    GCA_000021185  CP001175  type III  MSU  Finished",
            "title": "Getting the data"
        },
        {
            "location": "/tutorials/docs/pan_genome/#annotation-of-the-genomes",
            "text": "By annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.  prokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna  Annotate the 6 genomes, by replacing the  -outdir  and  -locustag  and  fasta file  accordingly.",
            "title": "Annotation of the genomes"
        },
        {
            "location": "/tutorials/docs/pan_genome/#pan-genome-analysis_1",
            "text": "put all the .gff files in the same folder (e.g., ./gff) and run Roary  roary -f results -e -n -v gff/*.gff  Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the  summary_statistics.txt  file.  Additionally, Roary produces a  gene_presence_absence.csv  file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.",
            "title": "Pan-genome analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#plotting-the-result",
            "text": "Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.  First, we need to generate a tree file from the alignment generated by Roary:  FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick  Then we can plot the Roary results:  roary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Plotting the result"
        },
        {
            "location": "/tutorials/docs/nanopore/",
            "text": "Introduction to Nanopore Sequencing\n\n\nIn this tutorial we will assemble the \nE. coli\n genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).\n\n\nThe MinION data used in this tutorial come a test run by the \nLoman lab\n.  \n\nThe Illumina data were simulated using \nInSilicoSeq\n\n\nGet the Data\n\n\nFirst download the nanopore data\n\n\nfastq-dump ERR1147227\n\n\n\n\nYou will not need the HiSeq data right away, but you can start the download in another window\n\n\ncurl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download\n\n\n\n\nlook at basic stats of the nanopore reads\n\n\nassembly-stats ERR1147227.fastq\n\n\n\n\n\n\nQuestion\n\n\nHow many nanopore reads do we have?\n\n\n\n\n\n\nQuestion\n\n\nHow long is the longest read?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the average read length?\n\n\n\n\nAdapter trimming\n\n\nWe'll use \nporechop\n to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.\n\n\nporechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq\n\n\n\n\nAssembly\n\n\nWe assemble the reads using miniasm\n\n\nminimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | \\\n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk '/^S/{print \">\"$2\"\\n\"$3}' ERR1147227.gfa | fold > ERR1147227.fasta\n\n\n\n\n\n\nNote\n\n\nMiniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.\n\n\n\n\nPolishing\n\n\nSince the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.\n\n\nFirst we map the short reads against the assembly\n\n\nbowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam\n\n\n\n\nthen we run Pilon\n\n\npilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved\n\n\n\n\nwhich will correct eventual misamatches in our assembly and write the new improved assembly to \nERR1147227_improved.fasta\n\n\nFor better results we should perform more than one round of polishing.\n\n\nCompare with the existing assembly\n\n\nGo to \nhttps://www.ncbi.nlm.nih.gov\n and search for NC_000913.\nDownload the associated genome in fasta format and rename it to \necoli_ref.fasta\n\n\nnucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta\n\n\n\n\nthen take a look at \necoli.png\n\n\nAnnotation\n\n\nawk '/^>/{print \">ctg\" ++i; next}{print}' < ERR1147227_improved.fasta \\\n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta\n\n\n\n\nYou can open the output to see how it went\n\n\ncat annotation/PROKKA_11232017.txt\n\n\n\n\n\n\nQuestion\n\n\nDoes it fit your expecations? How many genes were you expecting?",
            "title": "Nanopore Sequencing"
        },
        {
            "location": "/tutorials/docs/nanopore/#introduction-to-nanopore-sequencing",
            "text": "In this tutorial we will assemble the  E. coli  genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).  The MinION data used in this tutorial come a test run by the  Loman lab .   \nThe Illumina data were simulated using  InSilicoSeq",
            "title": "Introduction to Nanopore Sequencing"
        },
        {
            "location": "/tutorials/docs/nanopore/#get-the-data",
            "text": "First download the nanopore data  fastq-dump ERR1147227  You will not need the HiSeq data right away, but you can start the download in another window  curl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download  look at basic stats of the nanopore reads  assembly-stats ERR1147227.fastq   Question  How many nanopore reads do we have?    Question  How long is the longest read?    Question  What is the average read length?",
            "title": "Get the Data"
        },
        {
            "location": "/tutorials/docs/nanopore/#adapter-trimming",
            "text": "We'll use  porechop  to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.  porechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq",
            "title": "Adapter trimming"
        },
        {
            "location": "/tutorials/docs/nanopore/#assembly",
            "text": "We assemble the reads using miniasm  minimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | \\\n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk '/^S/{print \">\"$2\"\\n\"$3}' ERR1147227.gfa | fold > ERR1147227.fasta   Note  Miniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.",
            "title": "Assembly"
        },
        {
            "location": "/tutorials/docs/nanopore/#polishing",
            "text": "Since the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.  First we map the short reads against the assembly  bowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam  then we run Pilon  pilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved  which will correct eventual misamatches in our assembly and write the new improved assembly to  ERR1147227_improved.fasta  For better results we should perform more than one round of polishing.",
            "title": "Polishing"
        },
        {
            "location": "/tutorials/docs/nanopore/#compare-with-the-existing-assembly",
            "text": "Go to  https://www.ncbi.nlm.nih.gov  and search for NC_000913.\nDownload the associated genome in fasta format and rename it to  ecoli_ref.fasta  nucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta  then take a look at  ecoli.png",
            "title": "Compare with the existing assembly"
        },
        {
            "location": "/tutorials/docs/nanopore/#annotation",
            "text": "awk '/^>/{print \">ctg\" ++i; next}{print}' < ERR1147227_improved.fasta \\\n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta  You can open the output to see how it went  cat annotation/PROKKA_11232017.txt   Question  Does it fit your expecations? How many genes were you expecting?",
            "title": "Annotation"
        }
    ]
}